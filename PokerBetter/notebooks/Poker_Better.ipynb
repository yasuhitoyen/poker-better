{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsabWRxviz8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df81c0c-18cb-4d08-9f4f-96155c9a2ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting pettingzoo[classic]\n",
            "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[classic]) (1.2.0)\n",
            "Collecting chess>=1.9.4 (from pettingzoo[classic])\n",
            "  Downloading chess-1.11.2.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rlcard>=1.0.5 (from pettingzoo[classic])\n",
            "  Downloading rlcard-1.2.0.tar.gz (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.0/269.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[classic]) (2.6.1)\n",
            "Collecting shimmy>=1.2.0 (from shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic])\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[classic]) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[classic]) (0.0.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from rlcard>=1.0.5->pettingzoo[classic]) (3.1.0)\n",
            "Collecting open-spiel>=1.2 (from shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic])\n",
            "  Downloading open_spiel-1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic]) (24.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic]) (25.3.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic]) (1.4.0)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic]) (1.16.1)\n",
            "Collecting ml-collections>=0.1.1 (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic])\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from ml-collections>=0.1.1->open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"classic\"->pettingzoo[classic]) (6.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open_spiel-1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chess, rlcard\n",
            "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=2d5569966c5a7f7ea2f1103d39728383c6d859d62575048472ba3ebb7f48a18f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/5d/5c/59a62d8a695285e59ec9c1f66add6f8a9ac4152499a2be0113\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.2.0-py3-none-any.whl size=325795 sha256=49cb10e1c07bb1ce8c0a2cedcc986a349792dda334c3b662412a9bd2aa943db5\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/85/10/9ce42203776b06686ffe36d932983a79be6dfa95638ecf500b\n",
            "Successfully built chess rlcard\n",
            "Installing collected packages: rlcard, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-collections, chess, shimmy, pettingzoo, open-spiel, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed chess-1.11.2 ml-collections-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 open-spiel-1.6 pettingzoo-1.25.0 rlcard-1.2.0 shimmy-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"pettingzoo[classic]\" torch numpy tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will test out the Texas Holden No Limit v6 library to set up our play environment for both the user and model to play on.\n",
        "\n",
        "\n",
        "Actionn ID and Action\n",
        "0.   Fold\n",
        "1.   Check\n",
        "2.   Call\n",
        "3.   Raise Half Pot\n",
        "4.   Raise Full Pot\n",
        "5.   All In\n",
        "\n"
      ],
      "metadata": {
        "id": "cRXMLsCzQ0Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_map = {i: action for i, action in enumerate(['Fold', 'Check & Call', 'Raise Half Pot', 'Raise Full Pot', 'All In'])}"
      ],
      "metadata": {
        "id": "Ie3qO20hX0Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "card_map = {}\n",
        "\n",
        "ranks = ['A', '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K']\n",
        "suits = {\n",
        "    '♠': range(0, 13),\n",
        "    '♥': range(13, 26),\n",
        "    '◆': range(26, 39),\n",
        "    '♣': range(39, 52)\n",
        "}\n",
        "\n",
        "for suit, indices in suits.items():\n",
        "    for i, idx in enumerate(indices):\n",
        "        card_map[idx] = f\"{ranks[i]}{suit}\""
      ],
      "metadata": {
        "id": "wiRdJP3Bf3Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalLimiter():\n",
        "  def __init__(self, loop_limit: int=5):\n",
        "    self.loop_limit=loop_limit\n",
        "    self.counter=0\n",
        "  def inc(self):\n",
        "    self.counter += 1\n",
        "    if self.counter > self.loop_limit:\n",
        "        raise Exception('Exceeded loop limit for testing')"
      ],
      "metadata": {
        "id": "gTGQMofhWzIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALjxdO74lnrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for the users to properly visualize what is going on, we create a helper method to visualize the user's hands and the community cards"
      ],
      "metadata": {
        "id": "TLsrFoQKlqiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def visualize_cards(hand: List[int]):\n",
        "  for card, num_cards in enumerate(hand):\n",
        "    if num_cards:\n",
        "      for i in range(int(num_cards)):\n",
        "        print(f'{card_map[card]} ', end='')\n",
        "  print('\\n')\n",
        "\n",
        "# visualize_cards([2, 0])"
      ],
      "metadata": {
        "id": "VugOOd3nhxfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LB4-2uxwmNZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mtqEn_tYuPz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can simulate one hand per player for two players using the loop"
      ],
      "metadata": {
        "id": "tr_q3P2muRYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.classic import texas_holdem_no_limit_v6\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "env = texas_holdem_no_limit_v6.env()\n",
        "env.reset()\n",
        "\n",
        "gl = GlobalLimiter(loop_limit=5)\n",
        "turn = 1\n",
        "\n",
        "player_hands = {}\n",
        "\n",
        "for agent in env.agent_iter():\n",
        "  #gl.inc()\n",
        "  obs, reward, terminated, truncated, info = env.last()\n",
        "  done = terminated or truncated\n",
        "  player_cards = obs['observation'][:52]\n",
        "  players_commitment = obs['observation'][52:]\n",
        "\n",
        "\n",
        "  if done:\n",
        "    print(f'{agent} is done.')\n",
        "    if agent in player_hands:\n",
        "      visualize_cards(player_hands[agent])\n",
        "    env.step(None)\n",
        "    print(f'Cumulative Reward: {reward}')\n",
        "  else:\n",
        "    if int(np.sum(player_cards)) == 2:\n",
        "      player_hands[agent] = player_cards\n",
        "\n",
        "    legal_actions_indices = np.where(obs['action_mask'])[0]\n",
        "\n",
        "    print(f'\\n[{turn}] {agent} Turn:')\n",
        "    visualize_cards(player_cards)\n",
        "    print(f'Pot: {players_commitment}')\n",
        "    print(f'Available actions: {legal_actions_indices}')\n",
        "    action = random.choice(legal_actions_indices)\n",
        "    print(f'{agent} taking action: {action_map[action]} ({action})')\n",
        "    env.step(action)\n",
        "  #Returns observation, cumulative reward, terminated, truncated, info for the current agent (specified by self.agent_selection)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWWtF72kUHZt",
        "outputId": "30863e04-9143-4fca-dd47-418ad7a66f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1] player_1 Turn:\n",
            "3♥ Q◆ \n",
            "\n",
            "Pot: [1. 2.]\n",
            "Available actions: [0 1 3 4]\n",
            "player_1 taking action: All In (4)\n",
            "\n",
            "[1] player_0 Turn:\n",
            "T♥ T◆ \n",
            "\n",
            "Pot: [  2. 100.]\n",
            "Available actions: [0 1]\n",
            "player_0 taking action: Check & Call (1)\n",
            "player_0 is done.\n",
            "T♥ T◆ \n",
            "\n",
            "Cumulative Reward: -100\n",
            "player_1 is done.\n",
            "3♥ Q◆ \n",
            "\n",
            "Cumulative Reward: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from collections import deque, namedtuple\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done', 'history'])\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self,\n",
        "               capacity:int):\n",
        "    # FIFO buffer\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def push(self,\n",
        "           state:List[int],\n",
        "           action:int,\n",
        "           reward:int,\n",
        "           next_state:List[int],\n",
        "           done:int,\n",
        "           history\n",
        "           ):\n",
        "    self.buffer.append(Experience(state, action, reward, next_state, done, history))\n",
        "\n",
        "  def sample(self, batch_size:int):\n",
        "    return random.sample(self.buffer, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)"
      ],
      "metadata": {
        "id": "MbGQb7ow6k51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class FCNRewardFunction(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim:int,\n",
        "               hidden_dims:List[int] = [256, 128, 64]):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    prev_dim = input_dim\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "      layers.extend([\n",
        "          nn.Linear(in_features=prev_dim,\n",
        "                    out_features=hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=0.1)\n",
        "      ])\n",
        "      prev_dim=hidden_dim\n",
        "\n",
        "    # Add final layer to output single reward value\n",
        "    layers.append(nn.Linear(in_features=prev_dim, out_features=1))\n",
        "\n",
        "    self.network = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.network(x)"
      ],
      "metadata": {
        "id": "7TNlvLr_9MO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn\n",
        "from typing import List\n",
        "class HistoryRNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim:int,\n",
        "               hidden_dim:int=128,\n",
        "               num_layers:int=2,\n",
        "               dropout:float=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # store dims for init of hidden/cell states\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
        "                       num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "    self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "  def forward(self,\n",
        "              history_sequence:torch.Tensor,\n",
        "              sequence_lengths:List[int]):\n",
        "    # History sequence contains the actual sequence of prior game states\n",
        "    # Sequence lengths shows the true length of each game state prior to padding\n",
        "    # BTD Batch Size, Max History Length, Input feature size per timestep\n",
        "\n",
        "    # D is the size of every game snapshot that is to be consumed by RNN\n",
        "    batch_size = history_sequence.size(0)\n",
        "    seq_len = history_sequence.size(1)\n",
        "\n",
        "    if history_sequence.dim() == 2: # in case we only have one history\n",
        "      history_sequence = history_sequence.unsqueeze(0)\n",
        "      batch_size = 1\n",
        "\n",
        "    # hidden and cell state\n",
        "    h0 = torch.zeros(\n",
        "        self.num_layers, batch_size, self.hidden_dim).to(history_sequence.device)\n",
        "    c0 = torch.zeros(\n",
        "        self.num_layers, batch_size, self.hidden_dim).to(history_sequence.device)\n",
        "\n",
        "    if sequence_lengths is not None:\n",
        "      # accounts for padding since padding will pollute input with meaningless data\n",
        "      # normalize lengths to a plain python list of ints on CPU\n",
        "      if isinstance(sequence_lengths, torch.Tensor):\n",
        "        sequence_lengths = sequence_lengths.detach().cpu().tolist()\n",
        "      else:\n",
        "        sequence_lengths = [int(x) for x in sequence_lengths]\n",
        "\n",
        "      packed = rnn.pack_padded_sequence(\n",
        "          input=history_sequence, lengths=sequence_lengths, batch_first=True, enforce_sorted=False\n",
        "      )\n",
        "      # pass (packed, (h0, c0)) to the LSTM; don't call packed as a function\n",
        "      packed_output, (hn, cn) = self.rnn(packed, (h0, c0))\n",
        "      output, _ = rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "    else:\n",
        "      # no need to pack since we only have one example\n",
        "      output, (hn, cn) = self.rnn(history_sequence, (h0, c0))\n",
        "\n",
        "    history_encoding = self.output_projection(hn[-1])\n",
        "\n",
        "    return history_encoding\n"
      ],
      "metadata": {
        "id": "BmfeSMgF-6iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn  # alias nn so we can use nn.Sequential, nn.Linear, etc.\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self,\n",
        "               state_dim:int,\n",
        "               action_dim:int,\n",
        "               history_dim:int,\n",
        "               hidden_dim:int=256,\n",
        "               history_rnn_dim:int=128):\n",
        "    super().__init__()\n",
        "\n",
        "    # history rnn, history_dim is the size of each snapshot\n",
        "    self.history_rnn = HistoryRNN(history_dim, history_rnn_dim)\n",
        "\n",
        "    # state processor, input for \"what do i see right now?\"\n",
        "    self.state_processor = nn.Sequential(\n",
        "        nn.Linear(in_features=state_dim, out_features=hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout()\n",
        "    )\n",
        "\n",
        "    # combined dimension, combining state and history\n",
        "    combined_dim = hidden_dim + history_rnn_dim\n",
        "    combined_output_dim = hidden_dim // 2  # fix typo: combiend_output_dim -> combined_output_dim\n",
        "\n",
        "    # combined processor used for q head\n",
        "    self.combined_processor = nn.Sequential(\n",
        "        nn.Linear(in_features=combined_dim, out_features=hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(in_features=hidden_dim, out_features=combined_output_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # expected future return (now + later)\n",
        "    self.q_head = nn.Linear(\n",
        "        in_features=combined_output_dim, out_features=action_dim) # 5\n",
        "\n",
        "    # reward for current action, state\n",
        "    self.reward_function = FCNRewardFunction(input_dim=combined_dim) # 64\n",
        "\n",
        "  # forward will have the current state\n",
        "  def forward(self,\n",
        "              history,\n",
        "              state,\n",
        "              sequence_lengths,  # NEW: always pass sequence lengths so RNN can ignore padding\n",
        "              return_reward:bool=False):\n",
        "    # history: [batch, seq_length, history_dim]\n",
        "    history_vector = self.history_rnn(history, sequence_lengths)\n",
        "\n",
        "    state_vector = self.state_processor(state)\n",
        "\n",
        "    combined_vector = torch.cat((history_vector, state_vector), dim=1)\n",
        "\n",
        "    combined_embedding = self.combined_processor(combined_vector)\n",
        "\n",
        "    q_values = self.q_head(combined_embedding)\n",
        "\n",
        "    if return_reward:\n",
        "      fcn_reward = self.reward_function(combined_vector)\n",
        "      return q_values, fcn_reward\n",
        "\n",
        "    return q_values\n"
      ],
      "metadata": {
        "id": "QavV3uePBVMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, random\n",
        "from collections import deque\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "class DQLAgent:\n",
        "  def __init__(self, state_dim:int,\n",
        "               action_dim:int,\n",
        "               history_dim:int,\n",
        "               lr:float=1e-3,\n",
        "               gamma:float=0.99,\n",
        "               epsilon:float=1.0,\n",
        "               epsilon_decay:float=0.995,\n",
        "               epsilon_min:float=0.01,\n",
        "               buffer_size:int=10000,\n",
        "               batch_size:int=32,\n",
        "               maxlen:int=50):\n",
        "    # input dims\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.history_dim = history_dim\n",
        "\n",
        "    # hyper params\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    # neural networks\n",
        "    self.q_network = DQN(state_dim=state_dim, action_dim=action_dim,\n",
        "                              history_dim=history_dim)\n",
        "    self.target_network = DQN(state_dim=state_dim, action_dim=action_dim,\n",
        "                              history_dim=history_dim)\n",
        "\n",
        "    self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    self.optimizer = Adam(self.q_network.parameters(), lr=lr)  # not \"parameters=\"\n",
        "\n",
        "    self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "    self.history = deque(maxlen=maxlen)\n",
        "\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    self.q_network.to(self.device)\n",
        "    self.target_network.to(self.device)\n",
        "\n",
        "  def preprocess_state(self, obs):\n",
        "    player_cards = obs['observation'][:52]\n",
        "    players_commitment = obs['observation'][52:]\n",
        "\n",
        "    # redundant? check later\n",
        "\n",
        "    state = np.concatenate([player_cards, players_commitment])\n",
        "    return state\n",
        "\n",
        "  def add_to_history(self, obs):\n",
        "    state = self.preprocess_state(obs)\n",
        "    # redundant? check later\n",
        "    self.history.append(state)\n",
        "\n",
        "  def get_history_tensor(self):\n",
        "    if len(self.history) == 0:\n",
        "      return torch.zeros(1, 1, self.history_dim).to(self.device)\n",
        "    history_array = np.array(list(self.history))\n",
        "    # add batch dimension by unsqueezing\n",
        "    history_tensor = torch.FloatTensor(history_array).unsqueeze(0).to(self.device)\n",
        "    return history_tensor\n",
        "\n",
        "   # depending on the observation and legal actions, choose an action\n",
        "  def select_action(self, obs, legal_actions):\n",
        "    if random.random() < self.epsilon:\n",
        "      return random.choice(legal_actions)\n",
        "\n",
        "    self.add_to_history(obs)\n",
        "\n",
        "    state = torch.FloatTensor(self.preprocess_state(obs)).unsqueeze(0).to(self.device)\n",
        "    history = self.get_history_tensor()\n",
        "    # sequence lengths shows the true length of each game state prior to padding\n",
        "    sequence_lengths = [len(self.history) if len(self.history) > 0 else 1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      q_values = self.q_network(history, state, sequence_lengths)\n",
        "\n",
        "    masked_q_values = q_values.clone()\n",
        "    mask = torch.ones(self.action_dim, dtype=torch.bool, device=masked_q_values.device)\n",
        "    mask[legal_actions] = False\n",
        "    masked_q_values[0][mask] = float('-inf')\n",
        "\n",
        "    return masked_q_values.argmax().item()\n",
        "\n",
        "  def store_experience(self, state, action, reward, next_state, done, history):\n",
        "    self.replay_buffer.push(state, action, reward, next_state, done, history)\n",
        "\n",
        "  def update_target_network(self):\n",
        "    self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "  def train(self):\n",
        "    if len(self.replay_buffer) < self.batch_size:\n",
        "      return\n",
        "    ###### Digesting Data For Training #####\n",
        "    batch = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "    states = torch.FloatTensor(np.array([s.state for s in batch])).to(self.device)\n",
        "    actions = torch.LongTensor(np.array([s.action for s in batch])).to(self.device)\n",
        "    rewards = torch.FloatTensor(np.array([s.reward for s in batch])).to(self.device)\n",
        "    next_states = torch.FloatTensor(np.array([s.next_state for s in batch])).to(self.device)\n",
        "    dones = torch.BoolTensor(np.array([s.done for s in batch])).to(self.device)\n",
        "\n",
        "    # prepping (padding) histories\n",
        "    max_history_len = max(max(len(h) + 1, 1) for h in [s.history for s in batch])\n",
        "\n",
        "    histories, next_histories = [], []\n",
        "    sequence_lengths, next_sequence_lengths = [], []  # NEW: lengths for pack_padded_sequence\n",
        "\n",
        "    for s in batch:\n",
        "        hist = list(s.history)\n",
        "        # true (unpadded) length before padding\n",
        "        sequence_lengths.append(len(hist) if len(hist) > 0 else 1)\n",
        "\n",
        "        while len(hist) < max_history_len:\n",
        "            hist.insert(0, np.zeros(self.history_dim, dtype=np.float32))\n",
        "        histories.append(hist)\n",
        "\n",
        "        nhist = list(s.history)\n",
        "        nhist.append(s.next_state)\n",
        "        # true (unpadded) length for next history\n",
        "        next_len = len(nhist) if len(nhist) > 0 else 1\n",
        "        next_sequence_lengths.append(next_len)\n",
        "\n",
        "        while len(nhist) < max_history_len:\n",
        "            nhist.insert(0, np.zeros(self.history_dim, dtype=np.float32))\n",
        "        next_histories.append(nhist)\n",
        "\n",
        "    histories = torch.FloatTensor(np.array(histories)).to(self.device)\n",
        "    next_histories = torch.FloatTensor(np.array(next_histories)).to(self.device)\n",
        "\n",
        "    ##### Getting Q Values and rewards ######\n",
        "    current_q_values, predicted_rewards = self.q_network.forward(\n",
        "        histories, states, sequence_lengths=sequence_lengths, return_reward=True\n",
        "    )\n",
        "    current_q_values = current_q_values.gather(1, actions.unsqueeze(1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_q_values = self.target_network(\n",
        "          next_histories, next_states, sequence_lengths=next_sequence_lengths\n",
        "      )\n",
        "      target_q_values = rewards + (self.gamma * next_q_values.max(1)[0] * ~dones)\n",
        "\n",
        "    # calculate loss\n",
        "    q_loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
        "    reward_loss = F.mse_loss(predicted_rewards.squeeze(), rewards)\n",
        "\n",
        "    # combined loss, weighed heavy towards q\n",
        "    total_loss = q_loss + 0.1 * reward_loss  # Weight reward loss\n",
        "\n",
        "    # back prop\n",
        "    self.optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # epsilon decay\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
      ],
      "metadata": {
        "id": "jAIKOs4pNFZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_dql_agent(num_episodes:int=1000,\n",
        "                    target_update_freq:int=10):\n",
        "    env = texas_holdem_no_limit_v6.env()\n",
        "    env.reset()\n",
        "\n",
        "    # observation dimensions\n",
        "    first_agent = env.agent_selection\n",
        "    sample_obs, _, _, _, _ = env.last()\n",
        "\n",
        "    state_dim = len(sample_obs['observation'])\n",
        "    action_dim = len(sample_obs['action_mask'])\n",
        "    history_dim = state_dim\n",
        "\n",
        "    # initialize each agent\n",
        "    agents = {}\n",
        "    for agent_name in env.possible_agents:\n",
        "        agents[agent_name] = DQLAgent(state_dim, action_dim, history_dim)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        env.reset()\n",
        "        episode_rewards = {agent: 0 for agent in env.possible_agents}\n",
        "\n",
        "        # reset all histories, starting new game\n",
        "        for agent in agents.values():\n",
        "            agent.history.clear()\n",
        "\n",
        "        for agent_name in env.agent_iter():\n",
        "            obs, reward, terminated, truncated, info = env.last()\n",
        "\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if agent_name in agents:\n",
        "                episode_rewards[agent_name] += reward\n",
        "\n",
        "                if done:\n",
        "                    env.step(None)\n",
        "                else:\n",
        "                    # legal actions\n",
        "                    legal_actions = np.where(obs['action_mask'])[0].tolist()\n",
        "\n",
        "                    # previous experience\n",
        "                    agent = agents[agent_name]\n",
        "                    if hasattr(agent, 'prev_state'):\n",
        "                        next_state = agent.preprocess_state(obs)\n",
        "                        agent.store_experience(\n",
        "                            agent.prev_state, agent.prev_action, reward,\n",
        "                            next_state, done, list(agent.history)\n",
        "                        )\n",
        "\n",
        "                    # select action\n",
        "                    action = agent.select_action(obs, legal_actions)\n",
        "\n",
        "                    agent.prev_state = agent.preprocess_state(obs)\n",
        "                    agent.prev_action = action\n",
        "\n",
        "                    env.step(action)\n",
        "                    agent.train()\n",
        "            else:\n",
        "                env.step(None)\n",
        "\n",
        "        # after a certain number of episodes, update target network\n",
        "        if episode % target_update_freq == 0:\n",
        "            for agent in agents.values():\n",
        "                agent.update_target_network()\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            avg_rewards = {name: episode_rewards[name] for name in episode_rewards}\n",
        "            print(f\"Episode {episode}, Average Rewards: {avg_rewards}\")\n",
        "\n",
        "    return agents"
      ],
      "metadata": {
        "id": "xmCYrqQkw3PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_agents = None\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the DQL agents\n",
        "    trained_agents = train_dql_agent()\n",
        "\n",
        "    # Test with trained agents (modify your existing code)\n",
        "    env = texas_holdem_no_limit_v6.env()\n",
        "    env.reset()\n",
        "\n",
        "    for agent_name in env.agent_iter():\n",
        "        obs, reward, terminated, truncated, info = env.last()\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if done:\n",
        "            env.step(None)\n",
        "        else:\n",
        "            if agent_name in trained_agents:\n",
        "                legal_actions = np.where(obs['action_mask'])[0].tolist()\n",
        "                action = trained_agents[agent_name].select_action(obs, legal_actions) if len(legal_actions) > 0 else None\n",
        "            else:\n",
        "                # Fallback to random action\n",
        "                legal_actions = np.where(obs['action_mask'])[0].tolist()\n",
        "                action = random.choice(legal_actions) if len(legal_actions) > 0 else None\n",
        "\n",
        "            # Pretty-print action using action_map if available; otherwise just show the id\n",
        "            if action is None:\n",
        "                printable = \"NO_ACTION\"\n",
        "            else:\n",
        "                printable = action_map[action] if 'action_map' in globals() and isinstance(action_map, dict) and action in action_map else str(action)\n",
        "\n",
        "            print(f'{agent_name} taking action: {printable} ({action})')\n",
        "            env.step(action)\n"
      ],
      "metadata": {
        "id": "-XYEMLHlhGAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f45c92-3c39-4761-99f7-725b0acd9a53"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Average Rewards: {'player_0': np.int64(2), 'player_1': np.int64(-2)}\n",
            "Episode 100, Average Rewards: {'player_0': np.int64(-40), 'player_1': np.int64(40)}\n",
            "Episode 200, Average Rewards: {'player_0': np.int64(2), 'player_1': np.int64(-2)}\n",
            "Episode 300, Average Rewards: {'player_0': np.int64(-100), 'player_1': np.int64(100)}\n",
            "Episode 400, Average Rewards: {'player_0': np.int64(-100), 'player_1': np.int64(100)}\n",
            "Episode 500, Average Rewards: {'player_0': np.int64(-80), 'player_1': np.int64(80)}\n",
            "Episode 600, Average Rewards: {'player_0': np.int64(80), 'player_1': np.int64(-80)}\n",
            "Episode 700, Average Rewards: {'player_0': np.int64(80), 'player_1': np.int64(-80)}\n",
            "Episode 800, Average Rewards: {'player_0': np.int64(-100), 'player_1': np.int64(100)}\n",
            "Episode 900, Average Rewards: {'player_0': np.int64(-100), 'player_1': np.int64(100)}\n",
            "player_1 taking action: Raise Full Pot (3)\n",
            "player_0 taking action: Raise Half Pot (2)\n",
            "player_1 taking action: Raise Half Pot (2)\n",
            "player_0 taking action: Raise Half Pot (2)\n",
            "player_1 taking action: Raise Full Pot (3)\n",
            "player_0 taking action: Raise Full Pot (3)\n",
            "player_1 taking action: Raise Half Pot (2)\n",
            "player_0 taking action: All In (4)\n",
            "player_1 taking action: Check & Call (1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import random\n",
        "from pettingzoo.classic import texas_holdem_no_limit_v6\n",
        "\n",
        "# Action and card mappings\n",
        "action_map = {\n",
        "    0: 'Fold',\n",
        "    1: 'Check/Call',\n",
        "    2: 'Raise Half Pot',\n",
        "    3: 'Raise Full Pot',\n",
        "    4: 'All In'\n",
        "}\n",
        "\n",
        "# Fixed card mapping\n",
        "suits = ['♠', '♥', '♦', '♣']\n",
        "ranks = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A']\n",
        "\n",
        "def card_from_index(index):\n",
        "    \"\"\"Convert card index to readable card format\"\"\"\n",
        "    return ranks[index % 13] + suits[index // 13]\n",
        "\n",
        "# ---------- ASCII CARD RENDERER ----------\n",
        "\n",
        "def _card_ascii(rank: str, suit: str):\n",
        "    \"\"\"Return a single card as a list of 5 lines (ASCII box).\"\"\"\n",
        "    # Fit ranks to two chars (pad '10' look with 'T')\n",
        "    rank_left  = f\"{rank:<2}\"\n",
        "    rank_right = f\"{rank:>2}\"\n",
        "    center = f\"{suit:^9}\"\n",
        "    return [\n",
        "        \"┌─────────┐\",\n",
        "        f\"│{rank_left}       │\",\n",
        "        f\"│{center}│\",\n",
        "        f\"│       {rank_right}│\",\n",
        "        \"└─────────┘\",\n",
        "    ]\n",
        "\n",
        "def _card_back_ascii():\n",
        "    \"\"\"Facedown card as a list of 5 lines.\"\"\"\n",
        "    return [\n",
        "        \"┌─────────┐\",\n",
        "        \"│░░░░░░░░░│\",\n",
        "        \"│░░░░░░░░░│\",\n",
        "        \"│░░░░░░░░░│\",\n",
        "        \"└─────────┘\",\n",
        "    ]\n",
        "\n",
        "def render_cards_ascii(indices, pad_to=0):\n",
        "    \"\"\"\n",
        "    Render a horizontal row of cards from 0-51 indices.\n",
        "    Pads with facedown backs (?) up to pad_to cards if provided.\n",
        "    \"\"\"\n",
        "    cards = []\n",
        "    for i in indices:\n",
        "        r = ranks[i % 13]\n",
        "        s = suits[i // 13]\n",
        "        cards.append(_card_ascii(r, s))\n",
        "\n",
        "    # pad with backs\n",
        "    if pad_to and len(cards) < pad_to:\n",
        "        for _ in range(pad_to - len(cards)):\n",
        "            cards.append(_card_back_ascii())\n",
        "\n",
        "    if not cards:\n",
        "        # If nothing, show pad_to backs if requested, else one back\n",
        "        n = pad_to if pad_to else 1\n",
        "        cards = [_card_back_ascii() for _ in range(n)]\n",
        "\n",
        "    # stitch line-by-line\n",
        "    lines = []\n",
        "    for row in zip(*cards):\n",
        "        lines.append(\" \".join(row))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def render_unknown_ascii(n):\n",
        "    \"\"\"Render n facedown cards.\"\"\"\n",
        "    return \"\\n\".join([\" \".join(row) for row in zip(*([_card_back_ascii()] * n))])\n",
        "\n",
        "# ---------- GAME LOGIC ----------\n",
        "\n",
        "class PokerGame:\n",
        "    def __init__(self):\n",
        "        self.env = None\n",
        "        self.game_log = []\n",
        "        self.current_agent = None\n",
        "        self.game_over = False\n",
        "        self.pot_size = 0\n",
        "        self.player_chips = {}\n",
        "        self.round_count = 0\n",
        "        self.final_rewards = {}\n",
        "        # Track the latest 52-bit card vectors per player to infer community\n",
        "        self.last_cards_52 = {'player_0': None, 'player_1': None}\n",
        "        # Persist the final revealed board so it shows after the game ends\n",
        "        self.revealed_board_indices = None\n",
        "        self.reset_game()\n",
        "\n",
        "    def reset_game(self):\n",
        "        \"\"\"Reset the poker game\"\"\"\n",
        "        self.env = texas_holdem_no_limit_v6.env()\n",
        "        self.env.reset()\n",
        "        self.round_count += 1\n",
        "        self.game_log = [f\"🎰 **Round {self.round_count}** started!\"]\n",
        "        self.current_agent = None\n",
        "        self.game_over = False\n",
        "        self.pot_size = 0\n",
        "        self.player_chips = {'player_0': 200, 'player_1': 200}  # Starting chips\n",
        "        self.final_rewards = {}\n",
        "        self.last_cards_52 = {'player_0': None, 'player_1': None}\n",
        "        self.revealed_board_indices = None\n",
        "        return True\n",
        "\n",
        "    def compute_board_indices(self):\n",
        "        \"\"\"Compute community cards as intersection of both players' 52-bit card vectors.\"\"\"\n",
        "        a = self.last_cards_52['player_0']\n",
        "        b = self.last_cards_52['player_1']\n",
        "        if a is None or b is None:\n",
        "            return []\n",
        "        # Intersection where both see the card as present -> community cards\n",
        "        board_mask = np.logical_and(a, b)\n",
        "        board = [i for i, v in enumerate(board_mask) if v]\n",
        "        board.sort()\n",
        "        return board\n",
        "\n",
        "    def compute_hole_indices(self, player_key):\n",
        "        \"\"\"Compute player's hole cards as their cards minus the community.\"\"\"\n",
        "        vec = self.last_cards_52[player_key]\n",
        "        if vec is None:\n",
        "            return []\n",
        "        board = set(self.compute_board_indices())\n",
        "        mine = [i for i, v in enumerate(vec) if v and i not in board]\n",
        "        return mine[:2]\n",
        "\n",
        "    def get_player_cards(self, obs):\n",
        "        \"\"\"Extract and format player's hole cards (ASCII).\"\"\"\n",
        "        holes = self.compute_hole_indices('player_0')\n",
        "        if len(holes) == 2:\n",
        "            return render_cards_ascii(holes, pad_to=2)\n",
        "        # Fallback to current obs if cache not ready\n",
        "        if obs is not None:\n",
        "            cards = obs['observation'][:52]\n",
        "            player_cards = [i for i, val in enumerate(cards) if val == 1]\n",
        "            if len(player_cards) >= 2:\n",
        "                return render_cards_ascii(player_cards[:2], pad_to=2)\n",
        "        return render_cards_ascii([], pad_to=2)\n",
        "\n",
        "    def get_community_cards(self, obs=None):\n",
        "        \"\"\"Get community cards (ASCII) progressively; show 5 backs if unknown.\"\"\"\n",
        "        if self.revealed_board_indices is not None:\n",
        "            return render_cards_ascii(self.revealed_board_indices, pad_to=5)\n",
        "        board = self.compute_board_indices()\n",
        "        if len(board) == 0:\n",
        "            return render_cards_ascii([], pad_to=5)\n",
        "        # Show currently available community cards; pad to 5 with backs\n",
        "        return render_cards_ascii(board, pad_to=5)\n",
        "\n",
        "    def get_pot_info(self, obs):\n",
        "        \"\"\"Extract pot and betting information (approx via commitment slice).\"\"\"\n",
        "        if obs is None:\n",
        "            return \"💰 Pot: $0\"\n",
        "        commitment = obs['observation'][52:]\n",
        "        total_pot = float(sum(commitment)) if len(commitment) else 0.0\n",
        "        self.pot_size = total_pot\n",
        "        return f\"💰 Pot: ${total_pot:.0f}\"\n",
        "\n",
        "    def format_action(self, agent_name, action_idx):\n",
        "        \"\"\"Format action for display\"\"\"\n",
        "        action_name = action_map.get(action_idx, f\"Action {action_idx}\")\n",
        "        return f\"👤 **You**: {action_name}\" if agent_name == \"player_0\" else f\"🤖 **AI**: {action_name}\"\n",
        "\n",
        "    def determine_winner(self):\n",
        "        \"\"\"Determine the winner based on final rewards\"\"\"\n",
        "        if not self.final_rewards:\n",
        "            return \"🤷 No winner determined\"\n",
        "        player_reward = self.final_rewards.get('player_0', 0)\n",
        "        ai_reward = self.final_rewards.get('player_1', 0)\n",
        "        if player_reward > ai_reward:\n",
        "            return f\"🎉 **YOU WON!** (+${player_reward:.0f})\"\n",
        "        elif ai_reward > player_reward:\n",
        "            return f\"💔 **AI WON!** (You lost ${abs(player_reward):.0f})\"\n",
        "        else:\n",
        "            return \"🤝 **TIE GAME!**\"\n",
        "\n",
        "    def step_game(self, human_action=None):\n",
        "        \"\"\"Execute one step of the game\"\"\"\n",
        "        if self.game_over or self.env is None:\n",
        "            return self.get_game_state()\n",
        "\n",
        "        try:\n",
        "            current_obs = None\n",
        "            human_played = False\n",
        "            actions_this_step = []\n",
        "\n",
        "            for agent_name in self.env.agent_iter():\n",
        "                obs, reward, terminated, truncated, info = self.env.last()\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Cache latest 52-bit card vector for this agent\n",
        "                if obs is not None and 'observation' in obs and len(obs['observation']) >= 52:\n",
        "                    self.last_cards_52[agent_name] = np.array(obs['observation'][:52], dtype=bool)\n",
        "\n",
        "                if done:\n",
        "                    # Store final rewards\n",
        "                    self.final_rewards[agent_name] = reward\n",
        "                    self.env.step(None)\n",
        "\n",
        "                    # If both players are done, finalize game\n",
        "                    if all(self.env.terminations.values()) or all(self.env.truncations.values()):\n",
        "                        self.game_over = True\n",
        "                        # Persist final community cards\n",
        "                        final_board = self.compute_board_indices()\n",
        "                        self.revealed_board_indices = final_board\n",
        "\n",
        "                        # Log reveal\n",
        "                        board_str = render_cards_ascii(final_board, pad_to=5)\n",
        "                        self.game_log.append(\"🃏 **Final Community Cards (Flop • Turn • River)**\")\n",
        "                        self.game_log.append(board_str)\n",
        "\n",
        "                        winner_msg = self.determine_winner()\n",
        "                        self.game_log.append(\"=\" * 40)\n",
        "                        self.game_log.append(\"🏁 **GAME OVER**\")\n",
        "                        self.game_log.append(winner_msg)\n",
        "                        self.game_log.append(\"=\" * 40)\n",
        "                        break\n",
        "                    continue\n",
        "\n",
        "                # Store current observation for display\n",
        "                if agent_name == 'player_0':\n",
        "                    current_obs = obs\n",
        "\n",
        "                legal_actions = np.where(obs['action_mask'])[0].tolist()\n",
        "\n",
        "                # Human player (player_0)\n",
        "                if agent_name == 'player_0' and human_action is not None and not human_played:\n",
        "                    if human_action in legal_actions:\n",
        "                        action = human_action\n",
        "                        human_played = True\n",
        "                    else:\n",
        "                        action = legal_actions[0] if legal_actions else 0\n",
        "                        self.game_log.append(\"⚠️ **Invalid action!** Using default instead.\")\n",
        "                else:\n",
        "                    action = self.simple_ai_strategy(obs, legal_actions)\n",
        "\n",
        "                # Log the action\n",
        "                actions_this_step.append(self.format_action(agent_name, action))\n",
        "\n",
        "                # Execute action\n",
        "                self.env.step(action)\n",
        "\n",
        "                if human_played:\n",
        "                    break\n",
        "\n",
        "            # Add all actions from this step to the log\n",
        "            if actions_this_step:\n",
        "                self.game_log.extend(actions_this_step)\n",
        "                self.game_log.append(\"─\" * 30)\n",
        "\n",
        "            return self.get_game_state(current_obs)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.game_log.append(f\"❌ **Error**: {str(e)}\")\n",
        "            return self.get_game_state()\n",
        "\n",
        "    def simple_ai_strategy(self, obs, legal_actions):\n",
        "        \"\"\"Simple AI strategy for demo purposes\"\"\"\n",
        "        if not legal_actions:\n",
        "            return 0\n",
        "        if 0 in legal_actions and random.random() < 0.1:\n",
        "            return 0\n",
        "        if 1 in legal_actions and random.random() < 0.6:\n",
        "            return 1\n",
        "        remaining = [a for a in legal_actions if a != 0]\n",
        "        return random.choice(remaining) if remaining else legal_actions[0]\n",
        "\n",
        "    def get_game_state(self, obs=None):\n",
        "        \"\"\"Get current game state for display\"\"\"\n",
        "        display_log = [\"...\"] + self.game_log[-14:] if len(self.game_log) > 15 else self.game_log\n",
        "        log_display = \"\\n\".join(display_log)\n",
        "\n",
        "        player_cards_ascii = self.get_player_cards(obs)\n",
        "        community_cards_ascii = self.get_community_cards(obs)\n",
        "\n",
        "        pot_info = self.get_pot_info(obs) if obs else \"💰 Pot: $0\"\n",
        "\n",
        "        # Chip counts (roughly track commitments)\n",
        "        player_current = self.player_chips['player_0']\n",
        "        ai_current = self.player_chips['player_1']\n",
        "        if obs is not None:\n",
        "            commitment = obs['observation'][52:]\n",
        "            if len(commitment) >= 2:\n",
        "                player_current -= float(commitment[0])\n",
        "                ai_current -= float(commitment[1])\n",
        "        chip_info = f\"💰 **Your Chips**: ${player_current:.0f} | **AI Chips**: ${ai_current:.0f}\"\n",
        "\n",
        "        if obs and not self.game_over:\n",
        "            legal = np.where(obs['action_mask'])[0].tolist()\n",
        "            available_actions = [action_map[i] for i in legal]\n",
        "        else:\n",
        "            available_actions = list(action_map.values())\n",
        "\n",
        "        status = \"🏁 Game Over - Click 'New Game' to play again\" if self.game_over else \"🎯 Your turn - Choose an action\"\n",
        "\n",
        "        return (log_display, player_cards_ascii, community_cards_ascii,\n",
        "                f\"{pot_info}\\n{chip_info}\", available_actions, status)\n",
        "\n",
        "# Initialize game\n",
        "game = PokerGame()\n",
        "\n",
        "def play_action(action_choice):\n",
        "    \"\"\"Handle human player action\"\"\"\n",
        "    if action_choice is None or game.game_over:\n",
        "        return game.get_game_state()\n",
        "    # Convert action name to index\n",
        "    action_idx = None\n",
        "    for idx, name in action_map.items():\n",
        "        if name == action_choice:\n",
        "            action_idx = idx\n",
        "            break\n",
        "    if action_idx is not None:\n",
        "        return game.step_game(action_idx)\n",
        "    return game.get_game_state()\n",
        "\n",
        "def auto_play():\n",
        "    \"\"\"Auto-play one round (AI vs AI)\"\"\"\n",
        "    if game.game_over:\n",
        "        return game.get_game_state()\n",
        "    return game.step_game()\n",
        "\n",
        "def reset_game():\n",
        "    \"\"\"Reset the game\"\"\"\n",
        "    game.reset_game()\n",
        "    return game.get_game_state()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    title=\"🎰 Poker AI Game\",\n",
        "    css=\"\"\"\n",
        "    .game-container {\n",
        "        background: linear-gradient(135deg, #0f4c3a, #2d5a27);\n",
        "        border-radius: 15px;\n",
        "        padding: 20px;\n",
        "        margin: 10px;\n",
        "    }\n",
        "    .card-display {\n",
        "        background: #0b0e12;\n",
        "        color: #e7ecef;\n",
        "        border-radius: 10px;\n",
        "        padding: 12px;\n",
        "        font-family: 'Courier New', monospace;\n",
        "        font-size: 14px;\n",
        "        text-align: left;\n",
        "        border: 2px solid #4CAF50;\n",
        "        white-space: pre;           /* preserve ASCII layout */\n",
        "        line-height: 1.05;\n",
        "    }\n",
        "    .pot-display {\n",
        "        background: linear-gradient(135deg, #8b4513, #a0522d);\n",
        "        color: #ffd700;\n",
        "        border-radius: 10px;\n",
        "        padding: 15px;\n",
        "        font-weight: bold;\n",
        "        text-align: center;\n",
        "        border: 2px solid #ffd700;\n",
        "    }\n",
        "    .log-display {\n",
        "        background: #f8f9fa;\n",
        "        border-radius: 10px;\n",
        "        padding: 15px;\n",
        "        font-family: 'Arial', sans-serif;\n",
        "        line-height: 1.6;\n",
        "        border: 1px solid #dee2e6;\n",
        "        max-height: 300px;\n",
        "        overflow-y: auto;\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "    .status-display {\n",
        "        background: linear-gradient(135deg, #007bff, #0056b3);\n",
        "        color: white;\n",
        "        border-radius: 10px;\n",
        "        padding: 10px;\n",
        "        font-weight: bold;\n",
        "        text-align: center;\n",
        "        border: 2px solid #0056b3;\n",
        "    }\n",
        "    .action-btn {\n",
        "        background: #4CAF50;\n",
        "        color: white;\n",
        "        border-radius: 8px;\n",
        "        padding: 12px 24px;\n",
        "        font-weight: bold;\n",
        "        margin: 5px;\n",
        "    }\n",
        "    \"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🎰 Texas Hold'em Poker AI\n",
        "\n",
        "    **Play against an AI opponent in Texas Hold'em poker!**\n",
        "\n",
        "    Choose your action and watch the cards unfold. The AI will respond automatically.\n",
        "    Winners and losers are clearly displayed when the game ends!\n",
        "    \"\"\")\n",
        "\n",
        "    # Game status\n",
        "    status_display = gr.Textbox(\n",
        "        label=\"🎮 Game Status\",\n",
        "        value=\"🎯 Your turn - Choose an action\",\n",
        "        interactive=False,\n",
        "        elem_classes=[\"status-display\"]\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # Left column - Game controls\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🎮 Game Controls\")\n",
        "\n",
        "            action_dropdown = gr.Dropdown(\n",
        "                choices=list(action_map.values()),\n",
        "                label=\"Choose Your Action\",\n",
        "                value=\"Check/Call\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                play_btn = gr.Button(\"🎯 Play Action\", variant=\"primary\", elem_classes=[\"action-btn\"])\n",
        "                auto_btn = gr.Button(\"🤖 Auto Play\", variant=\"secondary\")\n",
        "\n",
        "            reset_btn = gr.Button(\"🔄 New Game\", variant=\"stop\", size=\"lg\")\n",
        "\n",
        "        # Right column - Game display\n",
        "        with gr.Column(scale=2):\n",
        "            # Community cards\n",
        "            community_display = gr.Textbox(\n",
        "                label=\"🛠️ Community Cards (Flop, Turn, River)\",\n",
        "                value=render_cards_ascii([], pad_to=5),\n",
        "                interactive=False,\n",
        "                lines=7,                   # taller to fit ASCII cards\n",
        "                elem_classes=[\"card-display\"]\n",
        "            )\n",
        "\n",
        "            # Pot and chip info\n",
        "            pot_display = gr.Textbox(\n",
        "                label=\"💰 Pot & Chip Status\",\n",
        "                value=\"💰 Pot: $0\\n💰 Your Chips: $200 | AI Chips: $200\",\n",
        "                interactive=False,\n",
        "                lines=2,\n",
        "                elem_classes=[\"pot-display\"]\n",
        "            )\n",
        "\n",
        "            # Player cards\n",
        "            player_cards = gr.Textbox(\n",
        "                label=\"🃏 Your Hole Cards\",\n",
        "                value=render_cards_ascii([], pad_to=2),\n",
        "                interactive=False,\n",
        "                lines=7,                   # taller to fit ASCII cards\n",
        "                elem_classes=[\"card-display\"]\n",
        "            )\n",
        "\n",
        "    # Game log with better styling\n",
        "    game_log = gr.Textbox(\n",
        "        label=\"📋 Game Action Log\",\n",
        "        value=\"🎰 New poker game started!\",\n",
        "        lines=10,\n",
        "        max_lines=12,\n",
        "        interactive=False,\n",
        "        elem_classes=[\"log-display\"]\n",
        "    )\n",
        "\n",
        "    # Available actions display\n",
        "    available_actions = gr.Textbox(\n",
        "        label=\"⚡ Available Actions\",\n",
        "        value=\", \".join(action_map.values()),\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # Event handlers\n",
        "    play_btn.click(\n",
        "        fn=play_action,\n",
        "        inputs=[action_dropdown],\n",
        "        outputs=[game_log, player_cards, community_display, pot_display, available_actions, status_display]\n",
        "    )\n",
        "\n",
        "    auto_btn.click(\n",
        "        fn=auto_play,\n",
        "        outputs=[game_log, player_cards, community_display, pot_display, available_actions, status_display]\n",
        "    )\n",
        "\n",
        "    reset_btn.click(\n",
        "        fn=reset_game,\n",
        "        outputs=[game_log, player_cards, community_display, pot_display, available_actions, status_display]\n",
        "    )\n",
        "\n",
        "    # Initialize display\n",
        "    demo.load(\n",
        "        fn=lambda: game.get_game_state(),\n",
        "        outputs=[game_log, player_cards, community_display, pot_display, available_actions, status_display]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "1BCcEXS-6BnJ",
        "outputId": "e0a52727-5291-4855-b5db-a4573ecaf7b2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://020639a51116b1a5e7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://020639a51116b1a5e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://020639a51116b1a5e7.gradio.live\n"
          ]
        }
      ]
    }
  ]
}